
# Week-1:
A Dynamic Dashboard in "Google Spreadsheets"
This Dashboard teels about Car Buyers, and the difference between the trends of married and single people, and also female and male.
#### The link to access the Dataset,Calculations and Dashboard is: https://docs.google.com/spreadsheets/d/1COos1CdGblaTFe-SMSM-fvUm5oxVmWuAJDOGCxZQFVI/edit?usp=sharing

# Week-2:

Week-2 Folder consistes of Frog-Leap_game code in Python created using

### 1.Lists

### 2.Slicing

### 3.Unicode strings

### 4.Loops

About Game:

There is always a chance for the user to move either the frog or invalid case until he "Quits". The user wins when all the left side frogs and right side frogs are exchanges their positions.
# Week-3
Week-3 Folder contains two Datasets named ---data_cleaning_for_beginners and chipotle files those are .csv and .tsv files.

The two datasets cleaning process is done using python and in the file named---Data_manipulation_using_two_datasets.py

The main libararies used are:

### 1.Numpy

### 2.Pandas

#### Direct link to my Google Colab Notebook : https://colab.research.google.com/drive/1bIifiytTQOpJDSjApll99puwufJfJ4G5?usp=sharing

# Week-4
Week-4 Folder Contains four datasets ----
#### 1. country_population.csv
#### 2. fertilty_rate.csv
#### 3. life_expectancy.csv
#### 4. Metadata_Country.csv
and also the --- python file of Visualizations of the data.

### Libraries Used are:

#### 1. Numpy
#### 2. Pandas
#### 3. Matplotlib
#### 4. Seaborn
#### 5. Plotly

Steps Followed are-------------
### Step-1:
1. Load all the four datasets into the environment and preprocesss all the datasets----Cleaning[ Deleting Null Values, Deleting Duplicates, Checking for inconsistencies in the data, melt the data for the consistency----there are 20+ years data so melt all the columns into a single Column.

### Step-2:
Merge all the datasets and form a single datset with---country code, year, population, fertility rate and life expectancy rate.

### Step-3:

1. Try some visualizations by plotting Line chart, Scatter pot, Distribution plot and Observe the variation of fertility rate.
2. We observe that Population is constantly oncreasing over the years.
3. Distribution of fertility rate for all the Regions.
### Step-4:
1. We observe the scatter plots for Region, Fertility Rate and Life Expectancy rate are not understandable and difficult to analyze.
2. So for tis problem we take the help of animation plots for the analysis of the data.

### Step-5:
1. Draw the visualizaton of animation group years for Region and population over years with Country Codes.
2. Draw the Animation Visualization for comparing Life Expectancy Rate and Fertility Rate.
3. Draw the Geographocal animation graph for Fretility rate with Asia and Life Expectancy rate with Asia.

#### Direct Link to the Google Colaboratory is: https://colab.research.google.com/drive/1RS6-WCXkiPr51tnsYLK8nxIx2ToA-D2x?usp=sharing

# Week-5
Week-5 folder contains projects on- SQL and Web Scrapping

### SQL Murder Mystery

#### Link to the SQL Murder Mystery Problem Statement: https://mystery.knightlab.com/

#### Problem Statement: A crime has taken place and the detective needs your help. The detective gave you the crime scene report, but you somehow lost it. You vaguely remember that the crime was a ​murder​ that occurred sometime on ​Jan.15, 2018​ and that it took place in ​SQL City​. Start by retrieving the corresponding crime scene report from the police department’s database.

#### Steps Involved:-

1. Load the SQL data into the colab and access the database.
2. After this, analyze the interviews given by the people and check the data in different tables.
3. After that we found the culprit --- "JEREMY BOWERS"
4. Check the interview of the culprit and match the details given in the database.
5. After Investigation, found Master Mind as "MIRANDA PRIESTLY"

### Web Scrapping

#### Link to find the Web page that is scrapped into a dataframe: https://books.toscrape.com/
#### Problem Statement: Scrap the given data into a dataframe in understandable format
##### Libraries used:   
#### 1.BeautifulSoup
#### 2.pandas

#### Steps Involved:-

1. Understand the format of the web page by inspecting the elements.
2. Using BeautifulSoup, send request as url and take response from the website.
3. There are 50 pages with 20 books in each page.
4. Using "find" find and store each element information in differnt columns.
5. Do this to all the pages available and combine the columns to form a dataframe.
6. The scrapping of the website involves only find and simple functions to convert them into dataframe.








